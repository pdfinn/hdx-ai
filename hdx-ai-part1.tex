% hdx-ai-part1.tex
% Introduction to GenAI Part 1: Data and Project Management (1 hour)
%
% Compile with: xelatex hdx-ai-part1.tex (run twice for TOC)
%
% HDX Beamer Theme - Happy Digital X | Tsinghua University

\documentclass[aspectratio=169]{beamer}

\usetheme{HDX}

% Additional packages
\usepackage{pifont}  % For \ding symbols (checkboxes, etc.)
\usepackage{tikz}    % For diagrams
\usepackage{hyperref} % For URLs
\usetikzlibrary{shapes,arrows,positioning,fit,calc,decorations.pathreplacing}

% Presentation metadata
\title{GenAI Data Foundations and Product Development}
\author{Happy Digital X}
\institute{Happy Digital X | Tsinghua University}
\date{\today}

\begin{document}

%% ============================================================================
%% TITLE SLIDE
%% ============================================================================

\begin{frame}[plain, noframenumbering]
    \titlepage
\end{frame}

%% ============================================================================
%% TABLE OF CONTENTS / AGENDA
%% ============================================================================

\begin{frame}{Today's Agenda}
    \textbf{Part 1: Data Foundations}
    \begin{itemize}
        \item How GenAI works: From GOFAI to neural networks
        \item Training, weights, and the ``black box'' problem
        \item Data quality, currency, and the 60--80\% rule
        \item Privacy regulations and governance frameworks
    \end{itemize}

    \vspace{5mm}

    \textbf{Part 2: Product Development}
    \begin{itemize}
        \item The GenAI hype cycle: Where we are now
        \item Project lifecycle and phase gates
        \item Build vs. buy decisions
        \item Success metrics and ROI reality
    \end{itemize}
\end{frame}

%% ============================================================================
%% SECTION 1: DATA FOUNDATIONS
%% ============================================================================

\section{Data Foundations}

%% --- Terminology: AI vs GenAI ---

\begin{frame}{A Note on Terminology}
    \begin{columns}[c]
        \begin{column}{0.55\textwidth}
            \textbf{AI (Artificial Intelligence)}\\
            The broad field of creating systems that perform tasks requiring human intelligence.

            \vspace{3mm}

            \textbf{Machine Learning (ML)}\\
            AI systems that learn patterns from data rather than following explicit rules.

            \vspace{3mm}

            \textbf{Generative AI (GenAI)}\\
            ML systems that \textit{create} new content: text, images, code, audio, video.
        \end{column}
        \begin{column}{0.4\textwidth}
            \begin{block}{In This Presentation}
                We use \textbf{``AI''} when discussing principles that apply broadly (e.g., bias, governance).

                \vspace{2mm}

                We use \textbf{``GenAI''} when discussing capabilities specific to generative systems (e.g., LLMs, hallucinations).
            \end{block}
        \end{column}
    \end{columns}
\end{frame}

%% --- Menti: Opening engagement ---

\begin{frame}{Quick Poll}
    \centering
    \vspace{5mm}

    {\Large\textbf{How would you rate your organization's\\data readiness for AI?}}

    \vspace{8mm}

    {\large Go to \textbf{menti.com} and enter the code}

    \vspace{5mm}

    {\Huge\textbf{[CODE]}}

    \vspace{5mm}

    \textit{1 = Not ready at all \quad 5 = Fully ready}
\end{frame}

{
\hdxpurplebg
\begin{frame}{The Fundamental Shift}
    \centering
    \vspace{10mm}

    {\Large\bfseries
    Traditional software is \textit{programmed}.\\[5mm]
    GenAI is \textit{trained}.}

    \vspace{10mm}

    This changes everything about how we build,\\test, and manage AI systems.
\end{frame}
}

\begin{frame}{Two Paradigms of Artificial Intelligence}
    \begin{columns}[c]
        \begin{column}{0.48\textwidth}
            \textbf{GOFAI: ``Good Old-Fashioned AI''}

            \vspace{2mm}

            \begin{itemize}
                \item Rules written by humans
                \item Symbolic reasoning
                \item Deterministic outputs
                \item Explainable decisions
                \item Brittle at edge cases
            \end{itemize}

            \vspace{2mm}

            \textit{Example}: Chess engines, expert systems, spell checkers
        \end{column}
        \begin{column}{0.48\textwidth}
            \textbf{Neural Networks (Connectionist)}

            \vspace{2mm}

            \begin{itemize}
                \item Patterns learned from data
                \item Statistical inference
                \item Probabilistic outputs
                \item Often opaque (``black box'')
                \item Flexible but unpredictable
            \end{itemize}

            \vspace{2mm}

            \textit{Example}: ChatGPT, image recognition, voice assistants
        \end{column}
    \end{columns}
\end{frame}

\begin{frame}{Example: A Medical Diagnosis Expert System}
    \centering
    \vspace{-3mm}
    \begin{tikzpicture}[scale=0.72, transform shape,
        node distance=0.7cm and 1.2cm,
        decision/.style={diamond, draw=hdxpurple, fill=hdxpurple!20, text width=1.1cm, align=center, inner sep=0pt, font=\scriptsize, aspect=1.2},
        outcome/.style={rectangle, rounded corners, draw=hdxgold, fill=hdxgold!20, text width=1.3cm, align=center, font=\scriptsize, minimum height=0.5cm},
        arrow/.style={->, thick, >=stealth}
    ]
        % Level 1 - Root
        \node[decision] (fever) {Fever?};

        % Level 2
        \node[decision, below left=0.8cm and 3cm of fever] (cough) {Cough?};
        \node[decision, below right=0.8cm and 3cm of fever] (pain) {Pain?};

        % Level 3
        \node[decision, below left=0.8cm and 1.2cm of cough] (phlegm) {Phlegm?};
        \node[decision, below right=0.8cm and 1.2cm of cough] (sore) {Sore\\Throat?};
        \node[decision, below left=0.8cm and 1.2cm of pain] (head) {Head?};
        \node[decision, below right=0.8cm and 1.2cm of pain] (chest) {Chest?};

        % Outcomes
        \node[outcome, below=0.7cm of phlegm] (bronch) {Bronchitis};
        \node[outcome, below=0.7cm of sore] (flu) {Influenza};
        \node[outcome, below=0.7cm of head] (migraine) {Migraine};
        \node[outcome, below=0.7cm of chest] (cardiac) {See Doctor};

        % Additional outcomes for No paths
        \node[outcome, right=0.6cm of bronch] (cold) {Cold};
        \node[outcome, right=0.6cm of migraine] (tension) {Tension};

        % Arrows Level 1 to 2
        \draw[arrow] (fever) -- node[above left, font=\tiny] {Yes} (cough);
        \draw[arrow] (fever) -- node[above right, font=\tiny] {No} (pain);

        % Arrows Level 2 to 3
        \draw[arrow] (cough) -- node[above left, font=\tiny] {Yes} (phlegm);
        \draw[arrow] (cough) -- node[above right, font=\tiny] {No} (sore);
        \draw[arrow] (pain) -- node[above left, font=\tiny] {Yes} (head);
        \draw[arrow] (pain) -- node[above right, font=\tiny] {No} (chest);

        % Arrows to outcomes
        \draw[arrow] (phlegm) -- node[left, font=\tiny] {Yes} (bronch);
        \draw[arrow] (phlegm.south east) -- node[right, font=\tiny, pos=0.3] {No} (cold.north);
        \draw[arrow] (sore) -- node[left, font=\tiny] {Yes} (flu);
        \draw[arrow] (head) -- node[left, font=\tiny] {Yes} (migraine);
        \draw[arrow] (head.south east) -- node[right, font=\tiny, pos=0.3] {No} (tension.north);
        \draw[arrow] (chest) -- node[right, font=\tiny] {Yes} (cardiac);
    \end{tikzpicture}
\end{frame}

\begin{frame}{GOFAI: Strengths and Limitations}
    \begin{columns}[t]
        \begin{column}{0.45\textwidth}
            \textbf{Strengths}

            \vspace{3mm}

            \begin{itemize}
                \item Transparent and explainable
                \item Predictable outputs
                \item Auditable for compliance
                \item No training data needed
            \end{itemize}
        \end{column}
        \begin{column}{0.45\textwidth}
            \textbf{Limitations}

            \vspace{3mm}

            \begin{itemize}
                \item Brittle at edge cases
                \item Doesn't learn from data
                \item Doesn't scale to complexity
                \item Can't handle ambiguity
            \end{itemize}
        \end{column}
    \end{columns}

    \vspace{8mm}

    \begin{block}{Key Insight}
        Expert systems encode \textit{what humans already know}. Neural networks discover patterns humans \textit{haven't articulated}.
    \end{block}
\end{frame}

\begin{frame}{How Neural Network Training Works}
    \textbf{High-Level Process:}

    \vspace{3mm}

    \begin{enumerate}
        \item \textbf{Collect Data}: Massive datasets (text, images, code, etc.)

        \item \textbf{Initialize}: Start with random ``weights'' (numerical parameters)

        \item \textbf{Train}: Show examples, adjust weights to reduce errors

        \item \textbf{Iterate}: Repeat billions of times across trillions of examples

        \item \textbf{Result}: A model that has learned patterns, not rules
    \end{enumerate}

    \vspace{3mm}

    \begin{alertblock}{Key Insight}
        The AI doesn't ``know'' anything---it has learned statistical patterns. When it generates text, it's predicting ``what word is likely to come next?''
    \end{alertblock}
\end{frame}

\begin{frame}{What Are ``Weights''? The Dial Analogy}
    \begin{columns}[c]
        \begin{column}{0.55\textwidth}
            Imagine a mixing board with \textbf{billions of dials}.

            \vspace{3mm}

            \begin{itemize}
                \item Each dial controls how much one piece of information influences another

                \item At first, all dials are set randomly---output is nonsense

                \item Training = adjusting dials slightly after each example

                \item After trillions of adjustments, the dials are tuned to produce useful output
            \end{itemize}

            \vspace{3mm}

            \textbf{GPT-4 reportedly has over 1 trillion parameters.}

            {\tiny (Exact architecture is proprietary)}
        \end{column}
        \begin{column}{0.4\textwidth}
            \begin{block}{The Black Box Problem}
                No human set these dials. No human can explain why dial \#847,293,102 is set to 0.0023.

                \vspace{2mm}

                The model works, but we can't fully explain \textit{why}.
            \end{block}
        \end{column}
    \end{columns}
\end{frame}

\begin{frame}{The Scale of Training Data}
    \textbf{Modern GenAI models are trained on unprecedented scale:}

    \vspace{3mm}

    \begin{itemize}
        \item \textbf{GPT-4}: Estimated 13+ trillion tokens of text
        \item \textbf{Image models}: Billions of image-text pairs
        \item \textbf{Code models}: Hundreds of billions of lines of code
    \end{itemize}

    \vspace{3mm}

    \hdxtwocolumn{
        \textbf{Strength}
        \begin{itemize}
            \item Broad knowledge
            \item Handles novel situations
            \item No manual rule-writing
            \item Learns nuance
        \end{itemize}
    }{
        \textbf{Weakness}
        \begin{itemize}
            \item Can't verify all data
            \item Absorbs biases
            \item \textbf{GIGO}: ``Garbage in, garbage out''
            \item Hard to ``unlearn''
        \end{itemize}
    }
\end{frame}

\begin{frame}{Chihuahua or Muffin?}
    \centering
    \vspace{-2mm}

    \includegraphics[height=0.78\textheight]{assets/stock/chihuahua-muffin.jpg}

    \vspace{1mm}

    {\tiny Image credit: @teenybiscuit / Karen Zack}
\end{frame}

\begin{frame}{Exercise: Try It Yourself}
    \textbf{Try This Now:}

    \vspace{5mm}

    \begin{enumerate}
        \item Open ChatGPT, Claude, or another GenAI

        \item Upload the chihuahua/muffin image

        \item Ask: ``How many dogs are in this image?''
    \end{enumerate}

    \vspace{8mm}

    \begin{block}{Discussion}
        What did you observe? Were the results what you expected?
    \end{block}
\end{frame}

%% --- Menti: AI dog count ---

\begin{frame}{Quick Poll}
    \centering
    \vspace{5mm}

    {\Large\textbf{How many dogs did your AI count?}}

    \vspace{8mm}

    {\large Go to \textbf{menti.com} and enter the code}

    \vspace{5mm}

    {\Huge\textbf{[CODE]}}
\end{frame}

\begin{frame}{What We Just Observed}
    \begin{columns}[c]
        \begin{column}{0.55\textwidth}
            \textbf{Key Takeaways from the Exercise:}

            \vspace{3mm}

            \begin{enumerate}
                \item \textbf{Different models, different answers}\\
                      Even leading AI systems disagree

                \item \textbf{Confidence without accuracy}\\
                      AI often sounds certain even when wrong

                \item \textbf{Edge cases expose limits}\\
                      Ambiguous inputs reveal brittleness

                \item \textbf{``Close enough'' isn't always enough}\\
                      Some applications require precision
            \end{enumerate}
        \end{column}
        \begin{column}{0.4\textwidth}
            \begin{alertblock}{The Lesson}
                GenAI excels at ``approximately right'' but struggles with ``exactly right.''

                \vspace{2mm}

                This has profound implications for how we deploy and validate AI systems.
            \end{alertblock}
        \end{column}
    \end{columns}
\end{frame}

\begin{frame}{Why Results Vary: Probabilistic, Not Deterministic}
    \textbf{Traditional Software:}
    \begin{itemize}
        \item Same input $\rightarrow$ Same output (always)
        \item 2 + 2 = 4, every time
    \end{itemize}

    \vspace{2mm}

    \textbf{GenAI:}
    \begin{itemize}
        \item Same input $\rightarrow$ \textit{Similar} output (usually)
        \item Outputs sampled from probability distributions
        \item ``Temperature'' controls randomness
        \item Even at temperature=0, results can vary
    \end{itemize}

    \vspace{2mm}

    \begin{alertblock}{Implication}
        You cannot test GenAI like traditional software. You need statistical evaluation over many samples.
    \end{alertblock}
\end{frame}

\begin{frame}{Hallucinations: When AI Makes Things Up}
    \textbf{What are hallucinations?}
    \begin{itemize}
        \item AI generates confident, plausible-sounding text that is \textbf{factually false}
        \item Not a bug---a fundamental feature of how LLMs work
    \end{itemize}

    \vspace{3mm}

    \textbf{Why do they happen?}
    \begin{itemize}
        \item LLMs predict the \textit{next most likely word}, not the \textit{true} word
        \item They have no internal model of truth---only patterns in training data
        \item They're designed to always give an answer, even when they shouldn't
    \end{itemize}

    \vspace{3mm}

    \begin{alertblock}{The Danger}
        Hallucinations are delivered with the same confidence as facts. Users often can't tell the difference without independent verification.
    \end{alertblock}
\end{frame}

\begin{frame}{Mitigating Hallucinations}
    \textbf{There is no complete solution, but these help:}

    \vspace{3mm}

    \begin{columns}[t]
        \begin{column}{0.48\textwidth}
            \textbf{Technical Approaches}
            \begin{itemize}
                \item \textbf{RAG}: Ground responses in retrieved documents
                \item \textbf{Fine-tuning}: Train on verified domain data
                \item \textbf{Temperature}: Lower values reduce creativity/risk
                \item \textbf{Structured outputs}: Constrain response format
            \end{itemize}
        \end{column}
        \begin{column}{0.48\textwidth}
            \textbf{Process Approaches}
            \begin{itemize}
                \item \textbf{Human review}: Verify critical outputs
                \item \textbf{Confidence thresholds}: Flag uncertain responses
                \item \textbf{Source citation}: Require references
                \item \textbf{Use case selection}: Avoid high-stakes facts
            \end{itemize}
        \end{column}
    \end{columns}

    \vspace{3mm}

    \begin{block}{Key Principle}
        Design systems assuming hallucinations will occur. The question is how to detect and handle them.
    \end{block}
\end{frame}

\begin{frame}{RLHF: Teaching AI to Be Helpful}
    \textbf{Reinforcement Learning from Human Feedback (RLHF)}

    \vspace{2mm}

    After initial training, models are refined using human preferences:

    \vspace{2mm}

    \begin{enumerate}
        \item Humans rate AI responses (helpful, harmless, honest)
        \item Model learns to maximize these ratings
        \item Creates more ``aligned'' behavior
    \end{enumerate}

    \vspace{3mm}

    \begin{columns}[t]
        \begin{column}{0.48\textwidth}
            \textbf{Benefits}
            \begin{itemize}
                \item Reduces harmful outputs
                \item Improves usefulness
                \item Adds safety guardrails
            \end{itemize}
        \end{column}
        \begin{column}{0.48\textwidth}
            \textbf{Risks}
            \begin{itemize}
                \item Evaluator biases transfer
                \item ``Sycophancy''---tells you what you want to hear
                \item Majority views dominate
            \end{itemize}
        \end{column}
    \end{columns}
\end{frame}

\begin{frame}{RLHF Bias: Who Trains the Trainers?}
    \begin{columns}[c]
        \begin{column}{0.55\textwidth}
            \textbf{The evaluators shape the AI:}

            \vspace{2mm}

            \begin{itemize}
                \item If evaluators are from one demographic, the model reflects their worldview

                \item If evaluators prefer polite over accurate, the model learns to be polite---even when wrong

                \item Minority perspectives can be systematically deprioritized

                \item Models can learn to \textit{manipulate} rather than genuinely help
            \end{itemize}
        \end{column}
        \begin{column}{0.4\textwidth}
            \begin{alertblock}{Example}
                A model trained by evaluators who dislike blunt answers will learn to soften bad news---even when clarity matters more than comfort.
            \end{alertblock}
        \end{column}
    \end{columns}
\end{frame}

\begin{frame}{Case Study: Replika's Feedback Loop Disaster}
    \begin{columns}[c]
        \begin{column}{0.55\textwidth}
            \textbf{Replika}: AI companion chatbot (2017--present)

            \vspace{2mm}

            \textbf{What Happened:}
            \begin{enumerate}
                \item Trained on 100M+ web dialogues
                \item Users could upvote/downvote responses
                \item Some users engaged in sexual roleplay
                \item AI learned this behavior got positive feedback
                \item AI began \textit{initiating} sexual content unprompted
            \end{enumerate}

            \vspace{2mm}

            \textbf{Result}: Reports of AI ``sexually harassing'' users, including minors. Italy banned the app in 2023.
        \end{column}
        \begin{column}{0.4\textwidth}
            \begin{alertblock}{Lessons}
                \begin{itemize}
                    \item Training data quality matters
                    \item Feedback loops amplify patterns
                    \item User behavior becomes model behavior
                    \item GIGO at scale
                \end{itemize}
            \end{alertblock}
        \end{column}
    \end{columns}
\end{frame}

\begin{frame}{Implications for Your Organization}
    \textbf{What This Means for GenAI Deployment:}

    \vspace{3mm}

    \begin{enumerate}
        \item \textbf{Testing is Different}: Statistical evaluation, not pass/fail

        \item \textbf{Edge Cases Are Unpredictable}: You can't enumerate all failure modes

        \item \textbf{Data Quality is Critical}: Your fine-tuning data shapes behavior

        \item \textbf{Feedback Loops Matter}: User interactions can shift model behavior

        \item \textbf{Bias is Inherited}: From training data \textit{and} from human evaluators

        \item \textbf{``Unlearning'' is Hard}: Removing problematic knowledge is technically difficult
    \end{enumerate}

    \vspace{3mm}

    \begin{block}{The Data Imperative}
        This is why we spend 60--80\% of GenAI project time on data preparation.
    \end{block}
\end{frame}

{
\hdxpurplebg
\begin{frame}{The Data Imperative}
    \centering
    \vspace{10mm}

    {\Large\bfseries
    ``Organizations don't have AI problems;\\[3mm]
    they have data problems that AI exposes.''}

    \vspace{10mm}

    Plan for 60--80\% of GenAI project time\\to be spent on data preparation.
\end{frame}
}

\begin{frame}{Data Strategy Precedes AI Strategy}
    \begin{columns}[c]
        \begin{column}{0.55\textwidth}
            \textbf{The Data Hierarchy of Needs:}

            \vspace{3mm}

            \begin{enumerate}
                \item \textbf{Data Collection} --- Foundation

                \item \textbf{Clean Data} --- Must start here

                \item \textbf{Analytics \& Reporting}

                \item \textbf{AI/ML} --- Most start here (mistake)
            \end{enumerate}

            \vspace{3mm}

            \begin{alertblock}{Reality Check}
                Fortune 500 expected 4 months for GenAI. Actual: 15 months. Root cause: Data readiness.

                \vspace{1mm}

                {\tiny Source: Industry case study analysis}
            \end{alertblock}
        \end{column}
        \begin{column}{0.4\textwidth}
            \includegraphics[width=\textwidth]{assets/stock/data-server.jpg}
        \end{column}
    \end{columns}
\end{frame}

\begin{frame}{Data Requirements for GenAI}
    \begin{itemize}
        \item \textbf{Training Data}: Building/fine-tuning models\\
              \textit{Strategic value: Competitive moat}

        \item \textbf{Context Data}: Grounding model outputs in your information\\
              Uses \textbf{RAG} (Retrieval-Augmented Generation): the AI retrieves relevant documents before generating a response\\
              \textit{Strategic value: Accuracy \& relevance}

        \item \textbf{Operational Data}: Real-time model inputs\\
              \textit{Strategic value: Timeliness}
    \end{itemize}
\end{frame}

\begin{frame}{The Data Quality Journey}
    \begin{columns}[c]
        \begin{column}{0.6\textwidth}
            \textbf{From Raw Data to AI-Ready:}

            \vspace{3mm}

            \begin{enumerate}
                \item \textbf{Raw Data} --- Unprocessed, unvalidated\\
                      \textit{``Rough diamond''---contains value but unusable as-is}

                \item \textbf{Cleaned Data} --- Errors removed, formats standardized\\
                      \textit{Deduplicated, consistent encoding}

                \item \textbf{Validated Data} --- Quality checked, business rules applied\\
                      \textit{Relationships verified, anomalies flagged}

                \item \textbf{AI-Ready Data} --- Labeled, balanced, documented\\
                      \textit{Ready for training or retrieval}
            \end{enumerate}
        \end{column}
        \begin{column}{0.35\textwidth}
            \begin{alertblock}{Reality}
                Most organizations are stuck at stage 1 or 2. This is why 60--80\% of GenAI project time is data preparation.
            \end{alertblock}
        \end{column}
    \end{columns}
\end{frame}

%% --- Menti: Data Quality Journey ---

\begin{frame}{Quick Poll}
    \centering
    \vspace{5mm}

    {\Large\textbf{Where is your organization on the\\data quality journey?}}

    \vspace{8mm}

    {\large Go to \textbf{menti.com} and enter the code}

    \vspace{5mm}

    {\Huge\textbf{[CODE]}}

    \vspace{5mm}

    \textit{1 = Raw Data \quad 2 = Cleaned \quad 3 = Validated \quad 4 = AI-Ready}
\end{frame}

\begin{frame}{Data Quality Dimensions}
    \begin{columns}[t]
        \begin{column}{0.48\textwidth}
            \textbf{Accuracy}\\
            Does the data reflect reality?\\
            \textit{Incorrect labels poison AI training}

            \vspace{3mm}

            \textbf{Completeness}\\
            Are there missing values or gaps?\\
            \textit{Missing data creates blind spots}

            \vspace{3mm}

            \textbf{Consistency}\\
            Same entity, same representation?\\
            \textit{``USA'' vs ``United States'' vs ``US''}
        \end{column}
        \begin{column}{0.48\textwidth}
            \textbf{Timeliness}\\
            Is the data current enough?\\
            \textit{Stale data = stale predictions}

            \vspace{3mm}

            \textbf{Representativeness}\\
            Does data reflect the real population?\\
            \textit{Biased samples = biased AI}

            \vspace{3mm}

            \textbf{Provenance}\\
            Where did this data come from?\\
            \textit{Can we trace and trust its origin?}
        \end{column}
    \end{columns}
\end{frame}

\begin{frame}{Data Currency: Time-Based Trust}
    \begin{columns}[c]
        \begin{column}{0.55\textwidth}
            \textbf{When was this data collected?}

            \vspace{2mm}

            Data has a ``shelf life'' that varies by domain:

            \vspace{2mm}

            \begin{itemize}
                \item \textbf{Stock prices}: Minutes to hours
                \item \textbf{News/events}: Hours to days
                \item \textbf{Product catalogs}: Days to weeks
                \item \textbf{Legal/regulatory}: Weeks to months
                \item \textbf{Scientific knowledge}: Months to years
            \end{itemize}

            \vspace{3mm}

            \begin{block}{Key Question}
                Is your training data still valid? Is your context data current?
            \end{block}
        \end{column}
        \begin{column}{0.4\textwidth}
            \begin{alertblock}{GenAI Risks}
                \begin{itemize}
                    \item Models trained on outdated data give outdated answers
                    \item RAG with stale documents misleads users
                    \item No timestamp = no way to assess trust
                \end{itemize}
            \end{alertblock}

            \vspace{2mm}

            \textit{Always know when your data was captured and when it expires.}
        \end{column}
    \end{columns}
\end{frame}

\begin{frame}{Global Privacy Regulations}
    \begin{columns}[c]
        \begin{column}{0.55\textwidth}
            \begin{itemize}
                \item \textbf{GDPR} (EU): Up to 4\% global revenue

                \item \textbf{CCPA/CPRA} (California): Per-violation penalties

                \item \textbf{PIPL} (China): Up to 5\% revenue

                \item \textbf{LGPD} (Brazil): Up to 2\% revenue

                \item \textbf{POPIA} (South Africa): Up to 10M ZAR
            \end{itemize}

            \vspace{3mm}

            \begin{block}{Global Trend}
                Design AI systems with privacy by default.
            \end{block}
        \end{column}
        \begin{column}{0.4\textwidth}
            \includegraphics[width=\textwidth]{assets/stock/privacy-data.jpg}
        \end{column}
    \end{columns}
\end{frame}

\begin{frame}{GenAI-Specific Privacy Concerns}
    \begin{enumerate}
        \item \textbf{Training Data Privacy}: Was personal data used with consent?

        \item \textbf{Inference Privacy}: Can model be manipulated to reveal data?

        \item \textbf{Output Privacy}: Do outputs contain personal information?

        \item \textbf{Conversation Privacy}: Who accesses user interactions?

        \item \textbf{Derived Data}: Are new personal insights generated?
    \end{enumerate}

    \vspace{3mm}

    \begin{alertblock}{The Consent Challenge}
        Traditional consent breaks down: capabilities hard to explain, data use unpredictable, untraining technically difficult.
    \end{alertblock}
\end{frame}

\begin{frame}{Data Governance Framework}
    \hdxtwocolumn{
        \textbf{Key Components}
        \begin{itemize}
            \item Data inventory \& classification
            \item Access controls
            \item Consent management
            \item Retention policies
            \item Audit trails
        \end{itemize}
    }{
        \textbf{Best Practices}
        \begin{itemize}
            \item Minimize data collection
            \item Purpose limitation
            \item Regular compliance audits
            \item Incident response plans
            \item Cross-border controls
        \end{itemize}
    }
\end{frame}

\begin{frame}{User Rights to Support}
    \begin{itemize}
        \item \textbf{Right to Access}: Users request all data held about them

        \item \textbf{Right to Erasure}: Users request deletion

        \item \textbf{Right to Portability}: Data in machine-readable format

        \item \textbf{Right to Rectification}: Correct inaccurate data

        \item \textbf{Right to Object}: Object to certain processing

        \item \textbf{Automated Decision Rights}: Human review of AI decisions
    \end{itemize}
\end{frame}

\begin{frame}{China's AI Regulatory Framework}
    \textbf{The world's most comprehensive AI regulations:}

    \vspace{3mm}

    \begin{itemize}
        \item \textbf{Algorithm Recommendations} (2022): Internet services

        \item \textbf{Deep Synthesis} (2023): Deepfakes, synthetic media

        \item \textbf{GenAI Service Measures} (2023): All public GenAI

        \item \textbf{AIGC Labeling} (Sept 2025): Mandatory AI content labels

        \item \textbf{National Standards} (Nov 2025): Security \& governance
    \end{itemize}

    \vspace{3mm}

    \textbf{Scale}: 350+ LLMs filed. 1.57M AI patents (38.6\% of global total).
\end{frame}

%% --- Transition: Data Foundations to Product Development ---

{
\hdxpurplebg
\begin{frame}{Section Summary: Data Foundations}
    \centering
    \vspace{5mm}

    {\Large\bfseries What We've Learned}

    \vspace{8mm}

    \begin{minipage}{0.9\textwidth}
        \begin{itemize}
            \item GenAI is \textbf{trained}, not programmed---data quality determines output quality

            \item Neural networks learn patterns humans haven't articulated, but inherit biases from training data and human evaluators

            \item Data has a \textbf{shelf life}---currency matters as much as quality

            \item Global privacy regulations (GDPR, PIPL, CCPA) create compliance obligations that AI doesn't eliminate
        \end{itemize}
    \end{minipage}

    \vspace{8mm}

    \textit{Next: How do we turn good data into successful AI products?}
\end{frame}
}

%% ============================================================================
%% SECTION 2: PRODUCT DEVELOPMENT
%% ============================================================================


\section{Product Development}

{
\hdxpurplebg
\begin{frame}{The GenAI Development Reality}
    \centering
    \vspace{5mm}

    {\Large\bfseries Key Statistics (2025)}

    \vspace{8mm}

    \begin{minipage}{0.85\textwidth}
        \begin{itemize}
            \item Only \textbf{5\%} of AI pilots achieve rapid revenue acceleration
            \item \textbf{67\%} success rate for purchasing/partnering
            \item \textbf{22\%} success rate for internal builds
            \item \textbf{46\%} have no structured ROI measurement
        \end{itemize}
    \end{minipage}

    \vspace{5mm}

    GenAI has entered the ``Trough of Disillusionment''

    \vspace{3mm}

    {\tiny Sources: MIT (2025), Wavestone (2025), Gartner Hype Cycle}
\end{frame}
}

\begin{frame}{How To Interpret A Hype Cycle}
    \centering
    \vspace{-2mm}

    \includegraphics[height=0.85\textheight]{assets/stock/gartner-hype-cycle.png}
\end{frame}

\begin{frame}{Understanding the Hype Cycle}
    \begin{columns}[c]
        \begin{column}{0.55\textwidth}
            \textbf{Five Phases of Technology Adoption:}

            \vspace{2mm}

            \begin{enumerate}
                \item \textbf{Innovation Trigger}\\
                      {\small Breakthrough generates excitement}

                \item \textbf{Peak of Inflated Expectations}\\
                      {\small Maximum hype, unrealistic promises}

                \item \textbf{Trough of Disillusionment}\\
                      {\small Reality sets in, failures mount}

                \item \textbf{Slope of Enlightenment}\\
                      {\small Practical understanding emerges}

                \item \textbf{Plateau of Productivity}\\
                      {\small Mainstream adoption, real value}
            \end{enumerate}
        \end{column}
        \begin{column}{0.4\textwidth}
            \begin{alertblock}{Where Is GenAI Now?}
                In 2024, GenAI was at the Peak.

                \vspace{2mm}

                In 2025, GenAI has descended into the \textbf{Trough of Disillusionment}.

                \vspace{2mm}

                This is \textit{normal}---not failure.
            \end{alertblock}
        \end{column}
    \end{columns}
\end{frame}

\begin{frame}{Quick Poll}
    \centering
    \vspace{5mm}

    {\Large\textbf{Why do you think AI adoption lags expectations?}}

    \vspace{8mm}

    {\large Go to \textbf{menti.com} and enter the code}

    \vspace{5mm}

    {\Huge\textbf{[CODE]}}

    \vspace{5mm}

    \textit{Share a word or short phrase.}
\end{frame}

\begin{frame}{Why Traditional Project Management Fails}
    \begin{columns}[c]
        \begin{column}{0.55\textwidth}
            \hdxtwocolumn{
                \textbf{Traditional}
                \begin{itemize}
                    \item Fixed requirements
                    \item Binary success
                    \item Predictable timeline
                    \item Deterministic testing
                \end{itemize}
            }{
                \textbf{GenAI}
                \begin{itemize}
                    \item Emergent requirements
                    \item Probabilistic success
                    \item Uncertain timeline
                    \item Statistical testing
                \end{itemize}
            }

            \vspace{2mm}

            \begin{alertblock}{Implication}
                Waterfall always fails. Agile is better but insufficient.
            \end{alertblock}
        \end{column}
        \begin{column}{0.4\textwidth}
            \includegraphics[width=\textwidth]{assets/stock/team-collaboration.jpg}
        \end{column}
    \end{columns}
\end{frame}

\begin{frame}{The AI Project Lifecycle}
    \begin{enumerate}
        \item \textbf{Problem Framing} (Often Skipped): Should AI solve this?

        \item \textbf{Data Assessment}: Inventory, gaps, quality

        \item \textbf{Proof of Concept} (4--8 weeks): Time-boxed experimentation

        \item \textbf{Pilot}: Limited production, controlled blast radius

        \item \textbf{Production \& Scale}: Infrastructure, monitoring

        \item \textbf{Operations}: Performance monitoring, retraining
    \end{enumerate}

    \vspace{3mm}

    \begin{block}{Rule of Thumb}
        Budget for 2--3 PoCs failing for every success.
    \end{block}
\end{frame}

\begin{frame}{Phase Gates for GenAI}
    \begin{itemize}
        \item \textbf{Gate 0}: Business case, feasibility, ethics screening

        \item \textbf{Gate 1}: Requirements, data availability, build vs. buy

        \item \textbf{Gate 2}: Technical validation, benchmarks, user feedback

        \item \textbf{Gate 3}: Production-grade, security \& ethics review

        \item \textbf{Gate 4}: Controlled deployment, monitoring setup

        \item \textbf{Gate 5}: Full deployment, continuous improvement
    \end{itemize}
\end{frame}

\begin{frame}{Kill Criteria: Define Before Starting}
    \begin{columns}[c]
        \begin{column}{0.55\textwidth}
            \begin{itemize}
                \item \textbf{Technical}: Can't achieve accuracy threshold

                \item \textbf{Economic}: Cost exceeds value

                \item \textbf{Timeline}: 6-month delay, no path forward

                \item \textbf{Ethical}: Can't mitigate bias

                \item \textbf{Security}: Can't protect data

                \item \textbf{Regulatory}: Unacceptable compliance risk

                \item \textbf{Strategic}: Market opportunity gone
            \end{itemize}
        \end{column}
        \begin{column}{0.4\textwidth}
            \includegraphics[width=\textwidth]{assets/stock/strategy-chess.jpg}
        \end{column}
    \end{columns}

    \vspace{2mm}

    \begin{alertblock}{Imperative}
        Establish kill criteria before emotional investment.
    \end{alertblock}
\end{frame}

\begin{frame}{Implementation Patterns}
    \begin{enumerate}
        \item \textbf{Co-Pilot / Augmentation}\\
              AI assists; humans decide. \textit{Best for: High-stakes, building trust}

        \item \textbf{Automation with Exceptions}\\
              AI handles routine; humans handle exceptions. \textit{Best for: High-volume}

        \item \textbf{Full Automation}\\
              AI autonomous with monitoring. \textit{Best for: Low-stakes, speed critical}

        \item \textbf{Internal Tool}\\
              AI assists employees only. \textit{Best for: Building capability, lower risk}
    \end{enumerate}
\end{frame}

\begin{frame}{Build vs. Buy Decision}
    \begin{itemize}
        \item \textbf{Build from Scratch}: \$10M--\$100M+; 12--24 months\\
              \textit{Only if: Massive data advantage}

        \item \textbf{Fine-Tune}: \$10K--\$1M; weeks to months\\
              \textit{Best for: Domain-specific tasks}

        \item \textbf{RAG (Retrieval-Augmented Generation)}: \$10K--\$100K; weeks\\
              \textit{Best for: Current/proprietary information}

        \item \textbf{Prompt Engineering}: \$1K--\$10K; days to weeks\\
              \textit{Best for: Quick wins}

        \item \textbf{Buy SaaS}: Variable; days\\
              \textit{Best for: Non-differentiating capabilities}
    \end{itemize}
\end{frame}

%% --- Menti: Build vs Buy ---

\begin{frame}{Quick Poll}
    \centering
    \vspace{5mm}

    {\Large\textbf{Which approach is your organization\\most likely to use?}}

    \vspace{8mm}

    {\large Go to \textbf{menti.com} and enter the code}

    \vspace{5mm}

    {\Huge\textbf{[CODE]}}

    \vspace{5mm}

    \textit{Build / Fine-Tune / RAG / Prompt Engineering / Buy SaaS}
\end{frame}

\begin{frame}{Success Metrics}
    \textbf{Avoid Vanity Metrics:}
    \begin{itemize}
        \item[\ding{55}] ``We deployed an AI model''
        \item[\ding{55}] ``95\% accuracy'' (on what?)
    \end{itemize}

    \vspace{3mm}

    \textbf{Focus on Business Outcomes:}
    \begin{itemize}
        \item[\ding{51}] Customer satisfaction improved by X\%
        \item[\ding{51}] Time to resolution decreased by Y hours
        \item[\ding{51}] Cost per transaction reduced by \$Z
        \item[\ding{51}] Employee time redirected to higher-value work
    \end{itemize}
\end{frame}

\begin{frame}{Four-Layer Monitoring Framework}
    \begin{enumerate}
        \item \textbf{Infrastructure}: Latency, error rates, throughput, cost

        \item \textbf{Model Performance}: Accuracy, hallucination rate, drift

        \item \textbf{Business}: Adoption, task completion, satisfaction, revenue

        \item \textbf{Risk}: Incidents, near-misses, compliance, complaints
    \end{enumerate}

    \vspace{3mm}

    \begin{block}{Principle}
        You can't improve what you don't measure. Monitor from day one.
    \end{block}
\end{frame}

\begin{frame}{ROI Reality (2025)}
    \begin{itemize}
        \item Average ROI: \textbf{3.7x} per dollar invested
        \item Top performers: \textbf{\$10.3} return per dollar
        \item 74\% meeting or exceeding expectations
        \item \textbf{46\% have no structured ROI measurement}
    \end{itemize}

    \vspace{3mm}

    \textbf{Timeline Expectations:}
    \begin{itemize}
        \item Chatbots, RPA: 6--12 months
        \item Operational efficiency: 12--24 months
        \item Revenue generation: 18--36 months
    \end{itemize}

    \vspace{2mm}

    {\tiny Sources: IDC/Microsoft (2025), Deloitte (2025), Wavestone (2025)}
\end{frame}

\begin{frame}{Total Cost of Ownership}
    \hdxtwocolumn{
        \textbf{Initial Costs}
        \begin{itemize}
            \item Infrastructure (GPUs)
            \item Software licenses
            \item Integration
            \item Data preparation
            \item Training
        \end{itemize}
    }{
        \textbf{Ongoing Costs}
        \begin{itemize}
            \item Compute resources
            \item API fees
            \item Model maintenance
            \item Monitoring
            \item Personnel
        \end{itemize}
    }

    \vspace{3mm}

    \textbf{Hidden Costs}: Compliance, legal/IP, incidents, technical debt, failed pilots
\end{frame}

\begin{frame}{Minimum Viable AI Team}
    \begin{itemize}
        \item \textbf{Executive Sponsor} (10--20\%): Alignment, resources, blockers

        \item \textbf{Product Owner} (Full-time): Requirements, prioritization

        \item \textbf{Data Engineer} (Full-time): Pipelines, quality

        \item \textbf{ML Engineer} (Full-time): Model development

        \item \textbf{Domain Expert} (25--50\%): Business logic, validation

        \item \textbf{MLOps Engineer}: Deployment, monitoring
    \end{itemize}
\end{frame}

%% ============================================================================
%% KEY TAKEAWAYS
%% ============================================================================

{
\hdxpurplebg
\begin{frame}{Part 1 Key Takeaways}
    \centering
    \vspace{5mm}

    {\Large\bfseries Summary}

    \vspace{8mm}

    \begin{minipage}{0.9\textwidth}
        \begin{enumerate}
            \item \textbf{Data First}: 60--80\% of GenAI time is data preparation

            \item \textbf{Privacy by Design}: Global regulations require it

            \item \textbf{Expect Failure}: Budget for 2--3 PoCs failing per success

            \item \textbf{Define Kill Criteria}: Before emotional investment

            \item \textbf{Measure Everything}: Connect to business outcomes

            \item \textbf{Build the Right Team}: Minimum viable AI team
        \end{enumerate}
    \end{minipage}
\end{frame}
}

\begin{frame}{Discussion Questions}
    \begin{enumerate}
        \item What is the current state of data readiness in your organization?

        \vspace{4mm}

        \item Have you defined clear kill criteria for your AI projects?

        \vspace{4mm}

        \item How are you measuring ROI on AI investments today?

        \vspace{4mm}

        \item Do you have the right team composition for AI success?
    \end{enumerate}
\end{frame}

%% ============================================================================
%% THANK YOU SLIDE
%% ============================================================================

\hdxthankyou
    {www.hdx.edu}
    {info@hdx.edu}
    {@HappyDigitalX}
    {Continue to Part 2: Ethics, Security \& Implementation}

\end{document}
